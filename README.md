# Brain-Computer-Interface-Music-Generation

This is code from an ongoing multi-disciplinary collaborative project through Georgia Tech.

I am working with the following people:

Professor - Dr. Francesco Fedele https://ce.gatech.edu/node/511

Music Technologist - Dr. Mike Winters https://mikewinters.io/about-me/

Artist - Rachel Grant https://www.rachelgrantstudio.com/

Dancer - Nadya Zeitlin https://www.nadyazeitlin.com/


We are innovating new ways to integrate science and art by using a consumer-grade EEG (Muse https://choosemuse.com/) to convert a person's brainwaves into music.

My role is to convert the raw brainwave data from the Muse into music. I did this by researching what each brainwave means, and deliberately connecting it to an appropriate element in the music (such as bass, harmony, melody, etc.).

Variations of this include reading Professor Fedele's brainwaves as he writes equations or Rachel's brainwaves as she draws a painting. Nadya then dances to the music generated by these brainwaves.

During performance, I set-up the signal flow as such: the signal from Muse is connected by Bluetooth to one's personal device. This signal is ported into Supercollider through the local network. Supercollider is an audio synthesis platform on which I wrote the code to translate the signal into music. This signal is then sent through a MIDI-bus to Ableton which generates selected soundscapes.

There are two versions of the code here:

One where the music depends on the amplitude of the brainwaves of a single person wearing a Muse.

Another where the music depends on the harmony of the brainwaves of two people who are each wearing a Muse.

Here is a link to our most recent performance: https://www.youtube.com/watch?v=jU2PflnULj0 I am in the middle, Dr. Fedele is writing equations on the board to the left, and Rachel is drawing a painting to the right.

This is a fun project, and we are curious and excited to see how it keeps evolving!
